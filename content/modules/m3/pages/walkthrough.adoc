# Setup Travel Demo `ServiceMeshControlPlane` for Production

== Task 1: Export Environment variables

[IMPORTANT,subs=attributes]
====
As this is a multi-tenant cluster you should restrict use for this lab to the following namespaces associated with your user *`{openshift_cluster_user_name}-prod-istio-system`*, *`{openshift_cluster_user_name}-prod-travel-control`*, *`{openshift_cluster_user_name}-prod-travel-portal`*, *`{openshift_cluster_user_name}-prod-travel-agency`*

Export the following in your link:https://codeserver-codeserver-{openshift_cluster_user_name}.{openshift_cluster_ingress_domain}[code-server] terminal and proceed to the location of the `ossm-labs` assets repository which you have cloned earlier.

[source,shell,subs=attributes,role=execute]
----
export CLUSTER_API={openshift_api_server_url}
export LAB_PARTICIPANT_ID={openshift_cluster_user_name}
export OCP_DOMAIN={openshift_cluster_ingress_domain}
export SSO_CLIENT_SECRET=bcd06d5bdd1dbaaf81853d10a66aeb989a38dd51
----
====

[NOTE,subs=attributes]
====
If you are running out of time and wish to complete the following lab sections in a single step execute

[source,shell,subs=attributes,role=execute]
----
cd lab-3
./complete-lab-3.sh {openshift_cluster_ingress_domain} {openshift_cluster_user_name}
----
====

== Task 2: Install a basic Service Mesh Control Plane with an external Jaeger configuration

You are going to create a basic Service Mesh Control Plane for production. Regarding the Tracing configuration, `Red Hat Openshift Service Mesh (OSSM)` makes the following 2 suggestions on setting up tracing for the production environment. `Option 2`, the _fully customized_ option, has been selected for the production setup.

- Option 1: link:https://docs.openshift.com/container-platform/4.14/service_mesh/v2x/ossm-deploy-production.html#ossm-smcp-prod_ossm-architecture[Production distributed tracing platform deployment (minimal) -  via SMCP Resource,window=_blank]
- Option 2: link:https://docs.openshift.com/container-platform/4.14/service_mesh/v2x/ossm-reference-jaeger.html#ossm-deploying-jaeger-production_jaeger-config-reference[Production distributed tracing platform deployment (fully customized),window=_blank]


Login as Mesh Operator (credentials: `emma/emma`) and run the `create-prod-smcp-1-tracing.sh` script (follow its output). This will deploy the production `SMCP` resource *`{openshift_cluster_user_name}-production`* and an external fully customized `Jaeger` instance in your lab user's Service Mesh controlplane namespace *`{openshift_cluster_user_name}-prod-istio-system`*.

[source,shell,subs=attributes,role=execute]
----
cd lab-3 
./login-as.sh emma
./create-prod-smcp-1-tracing.sh {openshift_cluster_user_name}-prod-istio-system {openshift_cluster_user_name}-production {openshift_cluster_user_name}-jaeger-small-production
----

* The `Jaeger Operator` will also create a `Jaeger Collector`, a `Jaeger Query` and an `Elastic Search` Deployment in your `{openshift_cluster_user_name}-prod-istio-system` in order to collect and backup the traces.
+
This is the Jaeger Custom Resource applied:
+
[source,yaml,subs=attributes]
----
kind: Jaeger
metadata:
  name:  {openshift_cluster_user_name}-jaeger-small-production
spec:
  strategy: production <1>
  storage:
    type: elasticsearch <2>
    esIndexCleaner:
      enabled: true
      numberOfDays: 7 <3>
      schedule: '55 23 * * *'
    elasticsearch:
      nodeCount: 1 <4>
      storage:
        size: 1Gi <5>
      resources:  <6>
        requests:
          cpu: 200m
          memory: 1Gi
        limits:
          memory: 1Gi
      redundancyPolicy: ZeroRedundancy <7>
----
The applied `Jaeger` setup will ensure that:

** *(1)* Production focused setup is applied
** *(2)* Backed up for persistence by Elastic Search
** *(3)* With indexes deleted every 7 days
** *(4)* Elastic Search will be hosted on a single Elastic node
** *(5)* Total Elastic Search Index size will be _`1Gi`_
** *(6)* Resource for the node will be both requested and limited
** *(7)* Since a single node is setup redundancy of the indices will be set to `ZeroRedundancy`


* This is the SMCP Resource that is configured to use the external Jaeger instance:
+
[source,yaml,subs=attributes]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: production
spec:
  security:
    dataPlane:
      automtls: true
      mtls: true
  tracing:
    sampling: 2000 <1>
    type: Jaeger
  general:
    logging:
      logAsJSON: true
  profiles:
    - default
  proxy:
    accessLogging:
      file:
        name: /dev/stdout
    networking:
      trafficControl:
        inbound: {}
        outbound:
          policy: REGISTRY_ONLY <2>
  policy:
    type: Istiod
  addons:
    grafana:
      enabled: true
    jaeger:  <3>
      install:
        ingress:
          enabled: true
        storage:
          type: Elasticsearch <4>
      name:  {openshift_cluster_user_name}-jaeger-small-production <5>
    kiali:
      enabled: true
    prometheus:
      enabled: true
  version: v2.5
  telemetry:
    type: Istiod"
----
+

The applied `ServiceMeshControlPlane` Resource ensures that:

** *(1)* 20% of all traces (as requested by the developers) will be collected,
** *(2)* No external outgoing communications to a host not registered in the mesh will be allowed,
** *(3)* `Jaeger` resource will be available in the `Service Mesh` for traces storage,
** *(4)* It will utilize Elastic Search for persistence of traces (unlike  in the `{openshift_cluster_user_name}-dev-istio-system` namespace where `memory` is utilized)
** *(5)* The `{openshift_cluster_user_name}-jaeger-small-production` external `Jaeger` Resource is integrated by and utilized in the `Service Mesh`.

Login to the Openshift console with Mesh Operator credentials `emma/emma` and navigate to *`Administrator`* -> *`Workloads`* -> *`Pods`*  in namespace `{openshift_cluster_user_name}-prod-istio-system` namespace. Verify all deployments and pods are running.

image::03-prod-istio-system.png[]

NOTE: The configs came from link:https://github.com/redhat-gpte-devopsautomation/ossm-labs/blob/main/lab-3/create-prod-smcp-1-tracing.sh[create-prod-smcp-1-tracing.sh,window=_blank] script which you can inspect for details.

== Task 3: Add the Application Namespaces to the Production Mesh and create the Deployments

In this task you will add the application namespaces to our newly created Service Mesh by specifying `ServiceMeshMember` resources and deploying the corresponding applications for production. You will also configure the applications for the usage within the Service Mesh by specifying two `sidecar` containers:

1. `istio-proxy` sidecar container: used to proxy all communications in/out of the main application container and apply `Service Mesh` configurations
2. `jaeger-agent` sidecar container: The `Service Mesh` documentation link:https://docs.openshift.com/container-platform/4.14/service_mesh/v2x/ossm-reference-jaeger.html#distr-tracing-deployment-best-practices_jaeger-config-reference[Jaeger Agent Deployment Best Practices,window=_blank] mentions the options of deploying `jaeger-agent` as sidecar or as `DaemonSet`. In order to allow `multi-tenancy` in this Openshift cluster the former has been selected.

All application `Deployment`(s) will be patched as follows to include the sidecars (*Warning:* Don't apply as the script `deploy-travel-services-domain.sh` further down will do so):

[source,shell,subs=attributes]
----
oc patch deployment/voyages -p '{"metadata":{"annotations":{"sidecar.jaegertracing.io/inject": " {openshift_cluster_user_name}-jaeger-small-production"}}}' -n {openshift_cluster_user_name}-prod-travel-portal
oc patch deployment/voyages -p '{"spec":{"template":{"metadata":{"annotations":{"sidecar.istio.io/inject": "true"}}}}}' -n $ENV-travel-portal
----

Now let's get started.

* Login as Mesh Developer (credentials `farid/farid`) who is responsible for the Travel Agency services and check the Labels for the `{openshift_cluster_user_name}-prod-travel-agency` application namespace
+
[source,shell,subs=attributes,role=execute]
----
./login-as.sh farid
./check-project-labels.sh {openshift_cluster_user_name}-prod-travel-agency
----
+
The result of this command should look similar to this:
+
[source,shell,subs=attributes]
----
{
  "kubernetes.io/metadata.name": "{openshift_cluster_user_name}-prod-travel-agency"
}
----

* Next add the application namespaces to the Production Service Mesh Tenant and check the Labels again
+
[source,shell,subs=attributes,role=execute]
----
./create-membership.sh {openshift_cluster_user_name}-prod-istio-system {openshift_cluster_user_name}-production {openshift_cluster_user_name}-prod-travel-agency

./check-project-labels.sh {openshift_cluster_user_name}-prod-travel-agency
----
+
The result of this command should look similar to this (you may need to retry a few times until all labels are applied):
+
[source,shell,subs=attributes]
----
{
  "kiali.io/member-of": "{openshift_cluster_user_name}-prod-istio-system",
  "kubernetes.io/metadata.name": "{openshift_cluster_user_name}-prod-travel-agency",
  "maistra.io/member-of": "{openshift_cluster_user_name}-prod-istio-system"
}
----

* Next you will deploy the Travel Agency Services applications and inject the sidecar containers.
+
[source,shell,subs=attributes,role=execute]
----
./deploy-travel-services-domain.sh prod prod-istio-system {openshift_cluster_user_name}
----
+
You can also login as `farid/farid` in the Openshift Console and verify the application PODs have started in your `{openshift_cluster_user_name}-prod-travel-agency` namespace (navigate to *`Administrator`* -> *`Workloads`* -> *`Pods`*). It should look like:
+
image::03-travel-agency-expected-3-container-pods.png[]


* In the next step you will install the second set of applications, the Travel Control and Travel Portal apps, with the responsible user `cristina/cristina`
+
[source,shell,subs=attributes,role=execute]
----
./login-as.sh cristina
./check-project-labels.sh {openshift_cluster_user_name}-prod-travel-control
./check-project-labels.sh {openshift_cluster_user_name}-prod-travel-portal
----

* Add the `{openshift_cluster_user_name}-prod-travel-control` application namespace to the Mesh
+
[source,shell,subs=attributes,role=execute]
----
./create-membership.sh {openshift_cluster_user_name}-prod-istio-system {openshift_cluster_user_name}-production {openshift_cluster_user_name}-prod-travel-control

./check-project-labels.sh {openshift_cluster_user_name}-prod-travel-control
----

* Add the `{openshift_cluster_user_name}-prod-travel-portal` application namespace to the Mesh
+
[source,shell,subs=attributes,role=execute]
----
./create-membership.sh {openshift_cluster_user_name}-prod-istio-system {openshift_cluster_user_name}-production {openshift_cluster_user_name}-prod-travel-portal

./check-project-labels.sh {openshift_cluster_user_name}-prod-travel-portal
----

* Next you will deploy the Travel Portal and Travel Control applications and inject the sidecars.
+
[source,shell,subs=attributes,role=execute]
----
./deploy-travel-portal-domain.sh prod prod-istio-system {openshift_cluster_ingress_domain} {openshift_cluster_user_name}
----

* Login with `cristina/cristina` in the Openshift Console and verify that the applications have been created and are running in the two namespaces:
** `{openshift_cluster_user_name}-prod-travel-control`
+
image::03-travel-control-expected-3-container-pods.png[]

** `{openshift_cluster_user_name}-prod-travel-portal`
+
image::03-travel-portal-expected-3-container-pods.png[]

== Task 4: Expose the Travel Portal Dashboard via TLS

After the deployment of the applications you will make them accessible outside of the cluster for the Travel Agency customers exposing the services with a custom TLS cert.
In order to achieve that,

* you are going to create a TLS certificate
* store it in a secret in our SMCP namespace
* create on Openshift passthrough route forwarding traffic to the Istio ingress Gateway
* create an Istio Gateway Resource configured with our TLS certificate

Right now if you login to the *production* link:https://kiali-{openshift_cluster_user_name}-prod-istio-system.{openshift_cluster_ingress_domain}/[Kiali Dashboard,window=_blank] with the user `emma/emma` (*Istio Config* -> filter by `VirtualService`) , there is an issue in the `VirtualService` resource `control` and an error on Kiali as no `Gateway` exists yet.

image::03-no-gw-for-travel-control-ui-vs.png[]

Login as Mesh Operator (credentials `emma/emma`) and execute the following script (follow the output) to achieve the above.

[source,shell,subs=attributes,role=execute]
----
./login-as.sh emma
./create-https-ingress-gateway.sh prod-istio-system {openshift_cluster_ingress_domain} {openshift_cluster_user_name}
----

NOTE: The configs come from link:https://github.com/redhat-gpte-devopsautomation/ossm-labs/blob/main/lab-3/create-https-ingress-gateway.sh[create-https-ingress-gateway.sh,window=_blank] script which you can inspect for details.

After finishing, the script above, you'll get the exposed URL Route and the `Travel Control Dashboard` should be accessible at https://travel-{openshift_cluster_user_name}.{openshift_cluster_ingress_domain} and the `Kiali` error on the `VirtualService` resource `control` should now have been resolved.

image::03-Travel-Control-Dashboard-https.png[Travel Control Dashboard]

== Task 5: Configure monitoring for Production

Currently (by default), the `mesh operator` deploys and manages all the monitoring addons as part of the link:https://console-openshift-console.{openshift_cluster_ingress_domain}/k8s/ns/{openshift_cluster_user_name}-prod-istio-system/clusterserviceversions/servicemeshoperator.v2.5.0/maistra.io{caret}v2{caret}ServiceMeshControlPlane/{openshift_cluster_user_name}-production[controlplane]. 

image::03-smcp-monitoring-stack.png[]

With a dedicated instance of Prometheus for collecting metrics, the `mesh operator` is responsible to provide resiliency for longterm storage of production metrics by extending the `Openshift Service Mesh Operator`.  There are several options to configure Prometheus to offer storage in production:

Option 1: Create a `PersistenceVolume` for the `SMCP` created `Prometheus` resource::
With this option the `mesh operator` will enhance the `SMCP` managed `Prometheus Deployment` resource in order to
* extend metric retention to 7 days (`7d`) and
* enable long-term persistence of the metrics by adding a persistent volume to the deployment.
Option 2: External `Prometheus` Setup via `prometheus-operator`::
With this option the `cluster admin` user will perform the following actions:
a. Deploy an additional `Prometheus Operator` in `prod-istio-system`
b. Deploy a `StatefulSet` based `Prometheus` resource with 2 replicas
c. Configure the prometheus replicas to monitor the components in `prod-istio-system` and all dataplane namespaces.
Option 3: Integrate with Openshift `Monitoring` Stack::
With this option metrics will be scraped by the Openshift Monitoring Stack's Prometheus and the changes required on the service mesh are described in link:https://docs.openshift.com/container-platform/4.14/service_mesh/v2x/ossm-observability.html#ossm-integrating-with-user-workload-monitoring_observability[Integrating with user-workload monitoring,window=_blank].
Option 4: Integrate with an external `Monitoring` Tool::
This option assumes that another tool like Datadog is used by the Operations team to collect metrics. In order to achieve this:

For the purpose of this lab you will deliver *Option 3* in the production setup. 

As a first step modify the Red Hat Openshift Console by adding the link:https://docs.openshift.com/container-platform/4.14/service_mesh/v2x/ossm-kiali-ossmc-plugin.html[OpenShift Service Mesh Console plugin] to view the production Kiali UI embedded directly from the Openshift Console *Service Mesh* menu. After applying the plugin (below) login to the link:{openshift_cluster_console_url}[Openshift Console] where after a few minutes you will be promted to refresh it. Look at the Openshift console menu (bottom left) for the *Service Mesh* menu where you will find links to the graphs, Istio Configs etc.

[source,shell,subs=attributes,role=execute]
----
./login-as.sh emma
echo "apiVersion: kiali.io/v1alpha1
kind: OSSMConsole
metadata:
  name: ossmconsole
  namespace: openshift-operators
spec:
  version: default
  kiali:
    serviceName: 'kiali'
    serviceNamespace: '{openshift_cluster_user_name}-prod-istio-system'" | oc apply -f -
----

Then, login as `Mesh Operator`, the script below will help you create the necessary configurations to allow metrics to be collected for controlplane and dataplane by the openshift monitoring stack.

[source,shell,subs=attributes,role=execute]
----
./login-as.sh emma
./update-prod-ocp-userworkload-monitoring.sh {openshift_cluster_user_name}
----

Add the `PodMonitor`, `ServiceMonitor` and `Telemetry` configurations as follows to complete the transition to Openshift user-workload monitoring stack.

[source,shell,subs=attributes,role=execute]
----
echo "apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: istio-proxies-monitor
  namespace: {openshift_cluster_user_name}-prod-travel-control
spec:
  selector:
    matchExpressions:
    - key: istio-prometheus-ignore
      operator: DoesNotExist
  podMetricsEndpoints:
  - path: /stats/prometheus
    interval: 30s
    relabelings:
    - action: keep
      sourceLabels: [__meta_kubernetes_pod_container_name]
      regex: "istio-proxy"
    - action: keep
      sourceLabels: [__meta_kubernetes_pod_annotationpresent_prometheus_io_scrape]
    - action: replace
      regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
      replacement: '[$2]:$1'
      sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_port,
      __meta_kubernetes_pod_ip]
      targetLabel: __address__
    - action: replace
      regex: (\d+);((([0-9]+?)(\.|$)){4})
      replacement: $2:$1
      sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_port,
      __meta_kubernetes_pod_ip]
      targetLabel: __address__
    - action: labeldrop
      regex: "__meta_kubernetes_pod_label_(.+)"
    - sourceLabels: [__meta_kubernetes_namespace]
      action: replace
      targetLabel: namespace
    - sourceLabels: [__meta_kubernetes_pod_name]
      action: replace
      targetLabel: pod_name
    - action: replace
#      replacement: {openshift_cluster_user_name}-prod-istio-system/{openshift_cluster_user_name}-production
      replacement: {openshift_cluster_user_name}-production-{openshift_cluster_user_name}-prod-istio-system
      targetLabel: mesh_id" |oc apply -f -

echo "apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: istio-proxies-monitor
  namespace: {openshift_cluster_user_name}-prod-travel-portal
spec:
  selector:
    matchExpressions:
    - key: istio-prometheus-ignore
      operator: DoesNotExist
  podMetricsEndpoints:
  - path: /stats/prometheus
    interval: 30s
    relabelings:
    - action: keep
      sourceLabels: [__meta_kubernetes_pod_container_name]
      regex: "istio-proxy"
    - action: keep
      sourceLabels: [__meta_kubernetes_pod_annotationpresent_prometheus_io_scrape]
    - action: replace
      regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
      replacement: '[$2]:$1'
      sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_port,
      __meta_kubernetes_pod_ip]
      targetLabel: __address__
    - action: replace
      regex: (\d+);((([0-9]+?)(\.|$)){4})
      replacement: $2:$1
      sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_port,
      __meta_kubernetes_pod_ip]
      targetLabel: __address__
    - action: labeldrop
      regex: "__meta_kubernetes_pod_label_(.+)"
    - sourceLabels: [__meta_kubernetes_namespace]
      action: replace
      targetLabel: namespace
    - sourceLabels: [__meta_kubernetes_pod_name]
      action: replace
      targetLabel: pod_name
    - action: replace
      replacement: {openshift_cluster_user_name}-production-{openshift_cluster_user_name}-prod-istio-system
      targetLabel: mesh_id" |oc apply -f -

echo "apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: istio-proxies-monitor
  namespace: {openshift_cluster_user_name}-prod-travel-agency
spec:
  selector:
    matchExpressions:
    - key: istio-prometheus-ignore
      operator: DoesNotExist
  podMetricsEndpoints:
  - path: /stats/prometheus
    interval: 30s
    relabelings:
    - action: keep
      sourceLabels: [__meta_kubernetes_pod_container_name]
      regex: "istio-proxy"
    - action: keep
      sourceLabels: [__meta_kubernetes_pod_annotationpresent_prometheus_io_scrape]
    - action: replace
      regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
      replacement: '[$2]:$1'
      sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_port,
      __meta_kubernetes_pod_ip]
      targetLabel: __address__
    - action: replace
      regex: (\d+);((([0-9]+?)(\.|$)){4})
      replacement: $2:$1
      sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_port,
      __meta_kubernetes_pod_ip]
      targetLabel: __address__
    - action: labeldrop
      regex: "__meta_kubernetes_pod_label_(.+)"
    - sourceLabels: [__meta_kubernetes_namespace]
      action: replace
      targetLabel: namespace
    - sourceLabels: [__meta_kubernetes_pod_name]
      action: replace
      targetLabel: pod_name
    - action: replace
      replacement: {openshift_cluster_user_name}-production-{openshift_cluster_user_name}-prod-istio-system
      targetLabel: mesh_id" |oc apply -f -

echo "apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: istio-proxies-monitor
  namespace: {openshift_cluster_user_name}-prod-istio-system
spec:
  selector:
    matchExpressions:
    - key: istio-prometheus-ignore
      operator: DoesNotExist
  podMetricsEndpoints:
  - path: /stats/prometheus
    interval: 30s
    relabelings:
    - action: keep
      sourceLabels: [__meta_kubernetes_pod_container_name]
      regex: "istio-proxy"
    - action: keep
      sourceLabels: [__meta_kubernetes_pod_annotationpresent_prometheus_io_scrape]
    - action: replace
      regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
      replacement: '[$2]:$1'
      sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_port,
      __meta_kubernetes_pod_ip]
      targetLabel: __address__
    - action: replace
      regex: (\d+);((([0-9]+?)(\.|$)){4})
      replacement: $2:$1
      sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_port,
      __meta_kubernetes_pod_ip]
      targetLabel: __address__
    - action: labeldrop
      regex: "__meta_kubernetes_pod_label_(.+)"
    - sourceLabels: [__meta_kubernetes_namespace]
      action: replace
      targetLabel: namespace
    - sourceLabels: [__meta_kubernetes_pod_name]
      action: replace
      targetLabel: pod_name
    - action: replace
      replacement: {openshift_cluster_user_name}-production-{openshift_cluster_user_name}-prod-istio-system
      targetLabel: mesh_id" |oc apply -f -
----

[source,shell,subs=attributes,role=execute]
----
echo "apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: istiod-monitor
  namespace: {openshift_cluster_user_name}-prod-istio-system
spec:
  targetLabels:
  - app
  selector:
    matchLabels:
      istio: pilot
  endpoints:
  - port: http-monitoring
    interval: 30s
    relabelings:
    - action: replace
#      replacement: {openshift_cluster_user_name}-prod-istio-system/{openshift_cluster_user_name}-production
      replacement: {openshift_cluster_user_name}-production-{openshift_cluster_user_name}-prod-istio-system
      targetLabel: mesh_id" |oc apply -f -
----

[source,shell,subs=attributes,role=execute]
----
echo "apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: enable-prometheus-metrics
  namespace: {openshift_cluster_user_name}-prod-istio-system
spec:
  metrics:
  - providers:
    - name: prometheus" |oc apply -f -
----


NOTE: The configs come from link:https://github.com/redhat-gpte-devopsautomation/ossm-labs/blob/main/lab-3/update-prod-ocp-userworkload-monitoring.sh[update-prod-ocp-userworkload-monitoring.sh,window=_blank] script which you can inspect for details.

After a few minutes the new components are up and running and the integration is complete. 

* Metrics can be viewed via the Openshift monitoring console (*Observe -> metrics*). Try some of the following metrics (more on link:https://github.com/redhat-developer-demos/ossm-heading-to-production-and-day-2/tree/main/scenario-8-mesh-tuning#what-to-monitor-in-the-data-plane[What to monitor in the data plane,window=_blank] and link:https://github.com/redhat-developer-demos/ossm-heading-to-production-and-day-2/tree/main/scenario-8-mesh-tuning#istiod-metrics-to-monitor[istiod metrics to monitor,window=_blank])):


[cols="a,a"]
|====
|Metric Purpose | Metric Query
| monitoring client latency averaged over the past minute by source and destination service names and namespace
|
[source, yaml]
----
histogram_quantile(0.95,
  sum(irate(istio_request_duration_milliseconds_bucket{reporter="source"}[1m]))
  by (
    destination_canonical_service,
    destination_workload_namespace,
    source_canonical_service,
    source_workload_namespace,
    le
  )
)
----

| monitoring for unsuccessful responses (or in absence try 200)
|
[source, yaml]
----
istio_request_duration_milliseconds_bucket{response_code="503"}
istio_request_duration_milliseconds_bucket{response_code="400"}
istio_request_duration_milliseconds_bucket{response_code="200"}
----

| monitor the time it takes for pilot to push new configurations to Envoy proxies (in milliseconds)
|
[source, yaml]
----
increase(pilot_proxy_convergence_time_sum[10m])/increase(pilot_proxy_convergence_time_count[10m])
----
|====

* Furthermore, the Kiali theconsole is still showing traces and metrics with the latter retrieved from Openshift monitoring stack.

== Task 6: Final Production Configuration

The following *Purpose* and *Principals* have been finalized with the `Travel Agency` architects and final `Service Mesh` configuration tunings have been accepted based on these:

* *Purpose:*
** Secure service-to-service communications.
** Monitor usage and health of the inter-service communications.
** Allow separate teams to work in isolation whilst delivering parts of a solution.
* *Principals:*
** An external mechanism of configuration of traffic encryption, authentication and authorization.
** Transparent integration of additional services of expanding functionality.
** An external traffic management and orchestration mechanism.
** All components will be configured with High Availability in mind.
** Observability is to be used for verification of system "sound operation", not auditing.

Therefore, based on these purpose and principals the final `PROD` setup will apply the following:

* _Tracing:_ used only for debug purposes (rather than as sensitive -auditing- information), a sample *5%* of all traces will only be collected, whilst these are going to be stored for *7 Days*. Elastic Search cluster will be used for this long-term storage.
* _Metrics:_ will have long-term storage (**7 Days**) with further archiving of the metrics beyond this period in order to assist historical comparisons
* _Grafana:_ will have persistance storage
* _Istio Ingress/Egress Gateways:_  (scale up to 2 instances)
* _Istiod Controlplane_ (scale up to 2 instances)

To apply the final production `SMCP` tuning, login as Mesh operator (credentials `emma/emma`) and execute the final update script. Follow the script logs to understand the changes applied. On a separate terminal you can execute `oc get pods -w -n {openshift_cluster_user_name}-prod-istio-system` to follow the POD scalings.

[source,shell,subs=attributes,role=execute]
----
./login-as.sh emma
./update-prod-smcp-3-final.sh {openshift_cluster_user_name}-prod-istio-system {openshift_cluster_user_name}-production {openshift_cluster_user_name}-jaeger-small-production
----

NOTE: The configs come from link:https://github.com/redhat-gpte-devopsautomation/ossm-labs/blob/main/lab-3/update-prod-smcp-3-final.sh[update-prod-smcp-3-final.sh,window=_blank] script which you can inspect for details.
