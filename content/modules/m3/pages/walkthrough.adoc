# Setup Travel Demo `ServiceMeshControlPlane` for Production

== Task 1: Export Environment variables

[IMPORTANT,subs=attributes]
====
As this is a multi-tenant cluster you should restrict use for this lab to the following namespaces associated with your user *`{openshift_cluster_user_name}-prod-istio-system`*, *`{openshift_cluster_user_name}-prod-travel-control`*, *`{openshift_cluster_user_name}-prod-travel-portal`*, *`{openshift_cluster_user_name}-prod-travel-agency`*

Export the following in your link:https://codeserver-codeserver-{openshift_cluster_user_name}.{openshift_cluster_ingress_domain}[code-server] terminal and proceed to the location of the `ossm-labs` assets repository which you have cloned earlier.

[source,shell,subs=attributes,role=execute]
----
export CLUSTER_API={openshift_api_server_url}
export LAB_PARTICIPANT_ID={openshift_cluster_user_name}
export OCP_DOMAIN={openshift_cluster_ingress_domain}
export SSO_CLIENT_SECRET=bcd06d5bdd1dbaaf81853d10a66aeb989a38dd51
----
====

[NOTE,subs=attributes]
====
If you are running out of time and wish to complete the following lab sections in a single step execute

[source,shell,subs=attributes,role=execute]
----
cd lab-3
./complete-lab-3.sh {openshift_cluster_ingress_domain} {openshift_cluster_user_name}
----
====

== Task 2: Install a basic Service Mesh Control Plane with an external Jaeger configuration

You are going to create a basic Service Mesh Control Plane for production. Regarding the Tracing configuration, `Red Hat Openshift Service Mesh (OSSM)` makes the following 2 suggestions on setting up tracing for the production environment. `Option 2`, the _fully customized_ option, has been selected for the production setup.

- Option 1: link:https://docs.openshift.com/container-platform/4.12/service_mesh/v2x/ossm-deploy-production.html#ossm-smcp-prod_ossm-architecture[Production distributed tracing platform deployment (minimal) -  via SMCP Resource,window=_blank]
- Option 2: link:https://docs.openshift.com/container-platform/4.12/service_mesh/v2x/ossm-reference-jaeger.html#ossm-deploying-jaeger-production_jaeger-config-reference[Production distributed tracing platform deployment (fully customized),window=_blank]


Login as Mesh Operator (credentials: `emma/emma`) and run the `create-prod-smcp-1-tracing.sh` script (follow its output). This will deploy the production `SMCP` resource *`{openshift_cluster_user_name}-production`* and an external fully customized `Jaeger` instance in your lab user's Service Mesh controlplane namespace *`{openshift_cluster_user_name}-prod-istio-system`*.

[source,shell,subs=attributes,role=execute]
----
cd lab-3 
./login-as.sh emma
./create-prod-smcp-1-tracing.sh {openshift_cluster_user_name}-prod-istio-system {openshift_cluster_user_name}-production {openshift_cluster_user_name}-jaeger-small-production
----

* The `Jaeger Operator` will also create a `Jaeger Collector`, a `Jaeger Query` and an `Elastic Search` Deployment in your `{openshift_cluster_user_name}-prod-istio-system` in order to collect and backup the traces.
+
This is the Jaeger Custom Resource applied:
+
[source,yaml,subs=attributes]
----
kind: Jaeger
metadata:
  name:  {openshift_cluster_user_name}-jaeger-small-production
spec:
  strategy: production <1>
  storage:
    type: elasticsearch <2>
    esIndexCleaner:
      enabled: true
      numberOfDays: 7 <3>
      schedule: '55 23 * * *'
    elasticsearch:
      nodeCount: 1 <4>
      storage:
        size: 1Gi <5>
      resources:  <6>
        requests:
          cpu: 200m
          memory: 1Gi
        limits:
          memory: 1Gi
      redundancyPolicy: ZeroRedundancy <7>
----
The applied `Jaeger` setup will ensure that:

** *(1)* Production focused setup is applied
** *(2)* Backed up for persistence by Elastic Search
** *(3)* With indexes deleted every 7 days
** *(4)* Elastic Search will be hosted on a single Elastic node
** *(5)* Total Elastic Search Index size will be _`1Gi`_
** *(6)* Resource for the node will be both requested and limited
** *(7)* Since a single node is setup redundancy of the indices will be set to `ZeroRedundancy`


* This is the SMCP Resource that is configured to use the external Jaeger instance:
+
[source,yaml,subs=attributes]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: production
spec:
  security:
    dataPlane:
      automtls: true
      mtls: true
  tracing:
    sampling: 2000 <1>
    type: Jaeger
  general:
    logging:
      logAsJSON: true
  profiles:
    - default
  proxy:
    accessLogging:
      file:
        name: /dev/stdout
    networking:
      trafficControl:
        inbound: {}
        outbound:
          policy: REGISTRY_ONLY <2>
  policy:
    type: Istiod
  addons:
    grafana:
      enabled: true
    jaeger:  <3>
      install:
        ingress:
          enabled: true
        storage:
          type: Elasticsearch <4>
      name:  {openshift_cluster_user_name}-jaeger-small-production <5>
    kiali:
      enabled: true
    prometheus:
      enabled: true
  version: v2.2
  telemetry:
    type: Istiod"
----
+

The applied `ServiceMeshControlPlane` Resource ensures that:

** *(1)* 20% of all traces (as requested by the developers) will be collected,
** *(2)* No external outgoing communications to a host not registered in the mesh will be allowed,
** *(3)* `Jaeger` resource will be available in the `Service Mesh` for traces storage,
** *(4)* It will utilize Elastic Search for persistence of traces (unlike  in the `dev-istio-system` namespace where `memory` is utilized)
** *(5)* The ` {openshift_cluster_user_name}-jaeger-small-production` external `Jaeger` Resource is integrated by and utilized in the `Service Mesh`.

Login to the Openshift console with Mesh Operator credentials `emma/emma` and navigate to *`Administrator`* -> *`Workloads`* -> *`Pods`*  in namespace `{openshift_cluster_user_name}-prod-istio-system` namespace. Verify all deployments and pods are running.

image::03-prod-istio-system.png[]

NOTE: The configs came from link:https://github.com/redhat-gpte-devopsautomation/ossm-labs/blob/main/lab-3/create-prod-smcp-1-tracing.sh[create-prod-smcp-1-tracing.sh,window=_blank] script which you can inspect for details.

== Task 3: Add the Application Namespaces to the Production Mesh and create the Deployments

In this task you will add the application namespaces to our newly created Service Mesh by specifying `ServiceMeshMember` resources and deploying the corresponding applications for production. You will also configure the applications for the usage within the Service Mesh by specifying two `sidecar` containers:

1. `istio-proxy` sidecar container: used to proxy all communications in/out of the main application container and apply `Service Mesh` configurations
2. `jaeger-agent` sidecar container: The `Service Mesh` documentation link:https://docs.openshift.com/container-platform/4.11/service_mesh/v2x/ossm-reference-jaeger.html#distr-tracing-deployment-best-practices_jaeger-config-reference[Jaeger Agent Deployment Best Practices,window=_blank] mentions the options of deploying `jaeger-agent` as sidecar or as `DaemonSet`. In order to allow `multi-tenancy` in this Openshift cluster the former has been selected.

All application `Deployment`(s) will be patched as follows to include the sidecars (*Warning:* Don't apply as the script `deploy-travel-services-domain.sh` further down will do so):

[source,shell,subs=attributes]
----
oc patch deployment/voyages -p '{"metadata":{"annotations":{"sidecar.jaegertracing.io/inject": " {openshift_cluster_user_name}-jaeger-small-production"}}}' -n $ENV-travel-portal
oc patch deployment/voyages -p '{"spec":{"template":{"metadata":{"annotations":{"sidecar.istio.io/inject": "true"}}}}}' -n $ENV-travel-portal
----

Now let's get started.

* Login as Mesh Developer (credentials `farid/farid`) who is responsible for the Travel Agency services and check the Labels for the `{openshift_cluster_user_name}-prod-travel-agency` application namespace
+
[source,shell,subs=attributes,role=execute]
----
./login-as.sh farid
./check-project-labels.sh {openshift_cluster_user_name}-prod-travel-agency
----
+
The result of this command should look similar to this:
+
[source,shell,subs=attributes]
----
{
  "kubernetes.io/metadata.name": "{openshift_cluster_user_name}-prod-travel-agency"
}
----

* Next add the application namespaces to the Production Service Mesh Tenant and check the Labels again
+
[source,shell,subs=attributes,role=execute]
----
./create-membership.sh {openshift_cluster_user_name}-prod-istio-system {openshift_cluster_user_name}-production {openshift_cluster_user_name}-prod-travel-agency

./check-project-labels.sh {openshift_cluster_user_name}-prod-travel-agency
----
+
The result of this command should look similar to this (you may need to retry a few times until all labels are applied):
+
[source,shell,subs=attributes]
----
{
  "kiali.io/member-of": "{openshift_cluster_user_name}-prod-istio-system",
  "kubernetes.io/metadata.name": "{openshift_cluster_user_name}-prod-travel-agency",
  "maistra.io/member-of": "{openshift_cluster_user_name}-prod-istio-system"
}
----

* Next you will deploy the Travel Agency Services applications and inject the sidecar containers.
+
[source,shell,subs=attributes,role=execute]
----
./deploy-travel-services-domain.sh prod prod-istio-system {openshift_cluster_user_name}
----
+
You can also login as `farid/farid` in the Openshift Console and verify the application PODs have started in your `{openshift_cluster_user_name}-prod-travel-agency` namespace (navigate to *`Administrator`* -> *`Workloads`* -> *`Pods`*). It should look like:
+
image::03-travel-agency-expected-3-container-pods.png[]


* In the next step you will install the second set of applications, the Travel Control and Travel Portal apps, with the responsible user `cristina/cristina`
+
[source,shell,subs=attributes,role=execute]
----
./login-as.sh cristina
./check-project-labels.sh {openshift_cluster_user_name}-prod-travel-control
./check-project-labels.sh {openshift_cluster_user_name}-prod-travel-portal
----

* Add the `{openshift_cluster_user_name}-prod-travel-control` application namespace to the Mesh
+
[source,shell,subs=attributes,role=execute]
----
./create-membership.sh {openshift_cluster_user_name}-prod-istio-system {openshift_cluster_user_name}-production {openshift_cluster_user_name}-prod-travel-control

./check-project-labels.sh {openshift_cluster_user_name}-prod-travel-control
----

* Add the `{openshift_cluster_user_name}-prod-travel-portal` application namespace to the Mesh
+
[source,shell,subs=attributes,role=execute]
----
./create-membership.sh {openshift_cluster_user_name}-prod-istio-system {openshift_cluster_user_name}-production {openshift_cluster_user_name}-prod-travel-portal

./check-project-labels.sh {openshift_cluster_user_name}-prod-travel-portal
----

* Next you will deploy the Travel Portal and Travel Control applications and inject the sidecars.
+
[source,shell,subs=attributes,role=execute]
----
./deploy-travel-portal-domain.sh prod prod-istio-system {openshift_cluster_ingress_domain} {openshift_cluster_user_name}
----

* Login with `cristina/cristina` in the Openshift Console and verify that the applications have been created and are running in the two namespaces:
** `{openshift_cluster_user_name}-prod-travel-control`
+
image::03-travel-control-expected-3-container-pods.png[]

** `{openshift_cluster_user_name}-prod-travel-portal`
+
image::03-travel-portal-expected-3-container-pods.png[]

== Task 4: Expose the Travel Portal Dashboard via TLS

After the deployment of the applications you will make them accessible outside of the cluster for the Travel Agency customers exposing the services with a custom TLS cert.
In order to achieve that,

* you are going to create a TLS certificate
* store it in a secret in our SMCP namespace
* create on Openshift passthrough route forwarding traffic to the Istio ingress Gateway
* create an Istio Gateway Resource configured with our TLS certificate

Right now if you login to the *production* link:https://kiali-{openshift_cluster_user_name}-prod-istio-system.{openshift_cluster_ingress_domain}/[Kiali Dashboard,window=_blank] with the user `emma/emma` (*Istio Config* -> filter by `VirtualService`) , there is an issue in the `VirtualService` resource `control` and an error on Kiali as no `Gateway` exists yet.

image::03-no-gw-for-travel-control-ui-vs.png[]

Login as Mesh Operator (credentials `emma/emma`) and execute the following script (follow the output) to achieve the above.

[source,shell,subs=attributes,role=execute]
----
./login-as.sh emma
./create-https-ingress-gateway.sh prod-istio-system {openshift_cluster_ingress_domain} {openshift_cluster_user_name}
----

NOTE: The configs come from link:https://github.com/redhat-gpte-devopsautomation/ossm-labs/blob/main/lab-3/create-https-ingress-gateway.sh[create-https-ingress-gateway.sh,window=_blank] script which you can inspect for details.

After finishing, the script above, you'll get the exposed URL Route and the `Travel Control Dashboard` should be accessible at https://travel-{openshift_cluster_user_name}.{openshift_cluster_ingress_domain} and the `Kiali` error on the `VirtualService` resource `control` should now have been resolved.

image::03-Travel-Control-Dashboard-https.png[Travel Control Dashboard]

== Task 5: Configure Prometheus for Production

In order to configure Prometheus for production there are several options:

Option 1: Create a `PersistenceVolume` for the `SMCP` created `Prometheus` resource::
With this option the `mesh operator` will enhance the `SMCP` managed `Prometheus Deployment` resource in order to
* extend metric retention to 7 days (`7d`) and
* enable long-term persistence of the metrics by adding a persistent volume to the deployment.
Option 2: External `Prometheus` Setup via `prometheus-operator`::
With this option the `cluster admin` user will perform the following actions:
a. Deploy an additional `Prometheus Operator` in `prod-istio-system`
b. Deploy a `StatefulSet` based `Prometheus` resource with 2 replicas
c. Configure the prometheus replicas to monitor the components in `prod-istio-system` and all dataplane namespaces.
Option 3: Integrate with Openshift `Monitoring` Stack::
With this option only the `dataplane` metrics (`istio-proxy` and business container) are collected. These will be scraped by the Openshift Monitoring Stack's Prometheus and the changes required on the service mesh are described in link:https://access.redhat.com/solutions/6958679[How to configure user-workload to monitor ServiceMesh application in Openshift 4,window=_blank].
Option 4: Integrate with an external `Monitoring` Tool::
This option assumes that another tool like Datadog is used by the Operations team to collect metrics. In order to achieve this:

a. For `controlplane` components metrics collection, the tool needs to be part of the control plane namespace or a `NetworkPolicy` to allow it visibility to those components is required.g
b. For `dataplane` metrics the same approach described, previously, in _Option 3_ is to be followed.

For the purpose of this lab you will deliver *Option 1* in the production setup. Login as `Mesh Operator` (credentials `emma/emma`), the script below will help you create a `PVC` for Prometheus and update the Prometheus configuration to utilize it and extend metrics retention to `168h`.

[source,shell,subs=attributes,role=execute]
----
./login-as.sh emma
./update-prod-smcp-2-prometheus.sh {openshift_cluster_user_name}-prod-istio-system
----

NOTE: The configs come from link:https://github.com/redhat-gpte-devopsautomation/ossm-labs/blob/main/lab-3/update-prod-smcp-2-prometheus.sh[update-prod-smcp-2-prometheus.sh,window=_blank] script which you can inspect for details.


== Task 6: Final Production Configuration

The following *Purpose* and *Principals* have been finalized with the `Travel Agency` architects and final `Service Mesh` configuration tunings have been accepted based on these:

* *Purpose:*
** Secure service-to-service communications.
** Monitor usage and health of the inter-service communications.
** Allow separate teams to work in isolation whilst delivering parts of a solution.
* *Principals:*
** An external mechanism of configuration of traffic encryption, authentication and authorization.
** Transparent integration of additional services of expanding functionality.
** An external traffic management and orchestration mechanism.
** All components will be configured with High Availability in mind.
** Observability is to be used for verification of system "sound operation", not auditing.

Therefore, based on these purpose and principals the final `PROD` setup will apply the following:

* _Tracing:_ used only for debug purposes (rather than as sensitive -auditing- information), a sample *5%* of all traces will only be collected, whilst these are going to be stored for *7 Days*. Elastic Search cluster will be used for this long-term storage.
* _Metrics:_ will have long-term storage (**7 Days**) with further archiving of the metrics beyond this period in order to assist historical comparisons
* _Grafana:_ will have persistance storage
* _Istio Ingress/Egress Gateways:_  (scale up to 2 instances)
* _Istiod Controlplane_ (scale up to 2 instances)

To apply the final production `SMCP` tuning, login as Mesh operator (credentials `emma/emma`) and execute the final update script. Follow the script logs to understand the changes applied. On a separate terminal you can execute `oc get pods -w -n {openshift_cluster_user_name}-prod-istio-system` to follow the POD scalings.

[source,shell,subs=attributes,role=execute]
----
./login-as.sh emma
./update-prod-smcp-3-final.sh {openshift_cluster_user_name}-prod-istio-system {openshift_cluster_user_name}-production {openshift_cluster_user_name}-jaeger-small-production
----

NOTE: The configs come from link:https://github.com/redhat-gpte-devopsautomation/ossm-labs/blob/main/lab-3/update-prod-smcp-3-final.sh[update-prod-smcp-3-final.sh,window=_blank] script which you can inspect for details.
